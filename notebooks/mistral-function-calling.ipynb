{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq langchain==0.1.10\n",
    "%pip install -qq pydantic==1.10.8\n",
    "%pip install -qq transformers==4.38.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IMPORTANT !!!\n",
    "Restart kerner after installing requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM Model Capable to Support Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"teknium/OpenHermes-2.5-Mistral-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, device=\"cuda:0\", torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class BookRecommendation(BaseModel):\n",
    "    \"\"\"Provides a book recommendation based on any specific interest.\"\"\"\n",
    "\n",
    "    interest: str = Field(\n",
    "        description=\"seeking a book suggestion tailored to a particular interest\"\n",
    "    )\n",
    "    recommended_book: str = Field(\n",
    "        description=\"book recommendations tailored to the manifested interests\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'BookRecommendation',\n",
       " 'description': 'Provides a book recommendation based on any specific interest.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'interest': {'description': 'seeking a book suggestion tailored to a particular interest',\n",
       "    'type': 'string'},\n",
       "   'recommended_book': {'description': 'book recommendations tailored to the manifested interests',\n",
       "    'type': 'string'}},\n",
       "  'required': ['interest', 'recommended_book']}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "convert_pydantic_to_openai_function(BookRecommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def extract_function_calls(completion):\n",
    "    completion = completion.strip()\n",
    "    pattern = r\"((.*?))\"\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    multiplefn = match.group(1)\n",
    "    root = ET.fromstring(multiplefn)\n",
    "    functions = root.findall(\"functioncall\")\n",
    "    return [json.loads(fn.text) for fn in functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_hermes(prompt, model, tokenizer, generation_config_overrides={}):\n",
    "    fn = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\"\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant with access to the following functions:\n",
    "\n",
    "{convert_pydantic_to_openai_function(BookRecommendation)}\n",
    "\n",
    "To use these functions respond with:\n",
    "\n",
    "     {fn} \n",
    "    ...\n",
    "\n",
    "\n",
    "Edge cases you must handle:\n",
    "- If there are no functions that match the user request, you will respond politely that you cannot help.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    generation_config = model.generation_config\n",
    "    generation_config.update(\n",
    "        **{\n",
    "            **{\n",
    "                \"use_cache\": True,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.2,\n",
    "                \"top_p\": 1.0,\n",
    "                \"top_k\": 0,\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"eos_token_id\": tokenizer.eos_token_id,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            },\n",
    "            **generation_config_overrides,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    n_tokens = inputs.input_ids.numel()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_tokens = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        generated_tokens.squeeze()[n_tokens:], skip_special_tokens=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizerFast' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommend me a book on Crime Thriller.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m---> 10\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mgeneration_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     functions \u001b[38;5;241m=\u001b[39m extract_function_calls(completion)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m functions:\n",
      "Cell \u001b[0;32mIn [41], line 21\u001b[0m, in \u001b[0;36mgenerate_hermes\u001b[0;34m(prompt, model, tokenizer, generation_config_overrides)\u001b[0m\n\u001b[1;32m      4\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_name\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: value_2, ...}}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|im_start|>system\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mYou are a helpful assistant with access to the following functions:\u001b[39m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<|im_end|>\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m<|im_start|>assistant\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m     generation_config \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m     22\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         }\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     38\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaTokenizerFast' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "generation_func = partial(generate_hermes, model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompts = [\n",
    "    \"Recommend me a book on Crime Thriller.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    completion = generation_func(prompt)\n",
    "    functions = extract_function_calls(completion)\n",
    "\n",
    "    if functions:\n",
    "        print(functions)\n",
    "    else:\n",
    "        print(completion.strip())\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
