{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai import caching\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ['GEMINI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"paper-of-the-week.txt\"\n",
    "file = genai.upload_file(path=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is ready ...\n",
      "File reference: files/frn3fib7inr1\n",
      "File url: https://generativelanguage.googleapis.com/v1beta/files/frn3fib7inr1\n"
     ]
    }
   ],
   "source": [
    "while file.state.name == \"PROCESSING\":\n",
    "    print(\"File is still processing. Waiting for 10 seconds...\")\n",
    "    time.sleep(10)\n",
    "    file = genai.get_file(file.name)\n",
    "\n",
    "print(f\"File is ready ...\")\n",
    "print(f\"File reference: {file.name}\")\n",
    "print(f\"File url: {file.uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = caching.CachedContent.create(\n",
    "    model=\"models/gemini-1.5-flash-001\",\n",
    "    display_name=\"Paper of the Week\",\n",
    "    system_instruction=\"You are an expert AI researcher, and your job is to answer users' queries based on the file you have access to.\",\n",
    "    contents=[file],\n",
    "    ttl=datetime.timedelta(minutes=15),\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest AI papers of the week are those published between **June 10th and June 16th, 2024**. You can find them listed under the heading  **\"Top ML Papers of the Week (June 10 - June 16) - 2024\"**  in the provided text. \n",
      "\n",
      "Here are the papers from that week, along with links to the paper and tweet where it was discussed:\n",
      "\n",
      "| **Paper**  | **Links** |\n",
      "| ------------- | ------------- |\n",
      "| 1) **Nemotron-4 340B** - provides an instruct model to generate high-quality data and a reward model to filter out data on several attributes; demonstrates strong performance on common benchmarks like MMLU and GSM8K; it’s competitive with GPT-4 on several tasks, including high scores in multi-turn chat; a preference data is also released along with the base model. | [Paper](https://research.nvidia.com/publication/2024-06_nemotron-4-340b), [Tweet](https://x.com/omarsar0/status/1802024352851878296) |\n",
      "| 2) **Discovering Preference Optimization Algorithms with LLMs** - proposes LLM-driven objective discovery of state-of-the-art preference optimization; no human intervention is used and an LLM is prompted to propose and implement the preference optimization loss functions based on previously evaluated performance metrics; discovers an algorithm that adaptively combined logistic and exponential losses.  | [Paper](https://arxiv.org/abs/2406.08414), [Tweet](https://x.com/SakanaAILabs/status/1801069076003082502) |\n",
      "| 3) **SelfGoal** - a framework to enhance an LLM-based agent's capabilities to achieve high-level goals; adaptively breaks down a high-level goal into a tree structure of practical subgoals during interaction with the environment; improves performance on various tasks, including competitive, cooperative, and deferred feedback environments  | [Paper](https://arxiv.org/abs/2406.04784), [Tweet](https://x.com/omarsar0/status/1800183982404829457) |\n",
      "| 4) **Mixture-of-Agents** - an approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents methodology; layers are designed with multiple LLM agents and each agent builds on the outputs of other agents in the previous layers; surpasses GPT-4o on AlpacaEval 2.0, MT-Bench and FLASK. | [Paper](https://arxiv.org/abs/2406.04692), [Tweet](https://x.com/togethercompute/status/1800536106729157054) |\n",
      "| 5) **Transformers Meet Neural Algorithmic Reasoners** - a new hybrid architecture that enables tokens in the LLM to cross-attend to node embeddings from a GNN-based neural algorithmic reasoner (NAR); the resulting model, called TransNAR, demonstrates improvements in OOD reasoning across algorithmic tasks | [Paper](https://arxiv.org/abs/2406.09308), [Tweet](https://x.com/omarsar0/status/1801448036389843228) |\n",
      "| 6) **Self-Tuning with LLMs** - improves an LLM’s ability to effectively acquire new knowledge from raw documents through self-teaching; the three steps involved are 1) a self-teaching component that augments documents with a set of knowledge-intensive tasks focusing on memorization, comprehension, and self-reflection, 2) uses the deployed model to acquire knowledge from new documents while reviewing its QA skills, and 3) the model is configured to continually learn using only the new documents which helps with thorough acquisition of new knowledge. | [Paper](https://arxiv.org/pdf/2406.06326),  [Tweet](https://x.com/omarsar0/status/1800552376513810463)  |\n",
      "| 7) **Sketching as a Visual Chain of Thought** - a framework that enables a multimodal LLM to access a visual sketchpad and tools to draw on the sketchpad; it can equip a model like GPT-4 with the capability to generate intermediate sketches to reason over complex tasks; improves performance on many tasks over strong base models with no sketching; GPT-4o equipped with SketchPad sets a new state of the art on all the tasks tested.  | [Paper](https://arxiv.org/abs/2406.09403),  [Tweet](https://x.com/omarsar0/status/1801450829234188760)  |\n",
      "| 8) **Mixture of Memory Experts** - proposes an approach to significantly reduce hallucination (10x) by tuning millions of expert adapters (e.g., LoRAs) to learn exact facts and retrieve them from an index at inference time; the memory experts are specialized to ensure faithful and factual accuracy on the data it was tuned on; claims to enable scaling to a high number of parameters while keeping the inference cost fixed.   | [Paper](https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf),  [Tweet](https://x.com/omarsar0/status/1801638552129700046)  |\n",
      "| 9) **Multimodal Table Understanding** - introduces Table-LLaVa 7B, a multimodal LLM for multimodal table understanding; it’s competitive with GPT-4V and significantly outperforms existing MLLMs on multiple benchmarks; also develops a large-scale dataset MMTab, covering table images, instructions, and tasks.  | [Paper](https://arxiv.org/abs/2406.08100), [Tweet](https://x.com/omarsar0/status/1801271773796716646)  |\n",
      "| 10) **Consistent Middle Enhancement in LLMs** - proposes an approach to tune an LLM to effectively utilize information from the middle part of the context; it first proposes a training-efficient method to extend LLMs to longer context lengths (e.g., 4K -> 256K); it uses a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning; the approach helps to alleviate the so-called \"Lost-in-the-Middle\" problem in long-context LLMs. | [Paper](https://arxiv.org/abs/2406.07138), [Tweet](https://x.com/omarsar0/status/1800903031736631473) |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Can you please tell me the latest AI papers of the week\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the papers that mention Mamba, along with their titles and summaries:\n",
      "\n",
      "**1) Top ML Papers of the Week (June 3 - June 9) - 2024**\n",
      "* **Paper:** **Mamba-2**\n",
      "* **Summary:**  A new architecture that combines state space models (SSMs) and structured attention. It uses 8x larger states and trains 50% faster. The new state space duality layer is more efficient and scalable compared to the approach used in Mamba. It also improves results on tasks that require large state capacity. \n",
      "\n",
      "**2) Top ML Papers of the Week (January 22 - January 28) - 2024**\n",
      "* **Paper:** **MambaByte** \n",
      "* **Summary:** Adapts Mamba SSM to learn directly from raw bytes. Bytes lead to longer sequences which autoregressive Transformers will scale poorly on. This work reports huge benefits related to faster inference and even outperforms subword Transformers.\n",
      "\n",
      "**3) Top ML Papers of the Week (January 15 - January 21) - 2024**\n",
      "* **Paper:** **MoE-Mamba**\n",
      "* **Summary:** An approach to efficiently scale LLMs by combining state space models (SSMs) with Mixture of Experts (MoE). MoE-Mamba, outperforms both Mamba and Transformer-MoE. It reaches the same performance as Mamba in 2.2x less training steps while preserving the inference performance gains of Mamba against the Transformer. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Can you list the papers that mention Mamba? List the title of the paper and summary.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the papers from the list related to GNNs: \n",
      "\n",
      "1. **Transformers Meet Neural Algorithmic Reasoners** - This paper proposes a hybrid architecture that combines the strengths of Transformers and Graph Neural Networks (GNNs). They introduce TransNAR, a model that enables tokens in the LLM to cross-attend to node embeddings from a GNN-based neural algorithmic reasoner (NAR). This approach aims to improve out-of-distribution (OOD) reasoning capabilities across algorithmic tasks.\n",
      "\n",
      "2. **GNN-RAG** - This paper combines the language understanding abilities of LLMs with the reasoning abilities of GNNs in a Retrieval-Augmented Generation (RAG) style. The GNN extracts relevant graph information, which is then used by the LLM to perform question answering over knowledge graphs (KGQA). The model demonstrates improved performance compared to standard LLMs on KGQA and surpasses or matches GPT-4 performance using a 7B tuned LLM.\n",
      "\n",
      "3. **Graph Machine Learning in the Era of LLMs** - This paper offers a comprehensive overview of recent advancements in Graph Machine Learning (GML) in the context of LLMs. It covers various aspects, including how LLMs enhance graph features and address challenges like out-of-distribution (OOD) generalization and graph heterogeneity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"What papers are related to GNNs? List the title of the paper and a summary.\")\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
