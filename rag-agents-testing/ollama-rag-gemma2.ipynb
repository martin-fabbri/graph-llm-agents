{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è | Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_LLM = \"gemma2\"\n",
    "LLM_MAX_TOKENS = 512\n",
    "LLM_TEMPERATURE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß | Embeddings (Do not show progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è | Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 in the <blogposts> collection.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "CHROMA_COLLECTION_NAME = \"blogposts\"\n",
    "CHROMA_PERSIST_DIRECTORY = \"./db-chromadb\"\n",
    "\n",
    "db = Chroma(\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "    embedding_function=embeddings,\n",
    "    # collection_name=CHROMA_COLLECTION_NAME\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"There are {db._collection.count()} in the <{CHROMA_COLLECTION_NAME}> collection.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• | Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 182 vectors in the <langchain> collection.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=300, add_start_index=True\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = db.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "\n",
    "print(\n",
    "    f\"There are {db._collection.count()} vectors in the <{db._collection.name}> collection.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì§ | Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† | LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=LOCAL_LLM,\n",
    "    keep_alive=\"3h\",\n",
    "    max_tokens=LLM_MAX_TOKENS,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ | Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPTS_PATH = \"prompts\"\n",
    "RAG_PROMPT_FILENAME = \"rag-prompt.jinja\"\n",
    "\n",
    "env = Environment(loader=FileSystemLoader(PROMPTS_PATH))\n",
    "template = env.get_template(RAG_PROMPT_FILENAME)\n",
    "prompt = ChatPromptTemplate.from_template(template.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the prompt for a runnable assign\n",
    "def print_prompt(input_dict):\n",
    "    formatted_prompt = prompt.format(**input_dict)\n",
    "    print(\"Generated Prompt:\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"-\" * 50)\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print and pass through the formatted prompt - string output\n",
    "def print_and_pass_prompt(formatted_prompt):\n",
    "    print(\"Generated Prompt:\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"-\" * 50)\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó | Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | print_and_pass_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí¨ | Chat with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    print(\"Answer:\", end=\" \", flush=True)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Generated Prompt:\n",
      "messages=[HumanMessage(content='Answer the question based only on the following context:\\n[Document(page_content=\\'where $\\\\\\\\hat{\\\\\\\\mathbf{x}}^a_\\\\\\\\theta (\\\\\\\\mathbf{z}_t), \\\\\\\\hat{\\\\\\\\mathbf{x}}^b_\\\\\\\\theta (\\\\\\\\mathbf{z}_t)$ are reconstructions of $\\\\\\\\mathbf{x}^a, \\\\\\\\mathbf{x}^b$ provided by the denoising model. And $w_r$ is a weighting factor and a large one $w_r >1$ is found to improve sample quality. Note that it is also possible to simultaneously condition on low resolution videos to extend samples to be at the high resolution using the same reconstruction guidance method.\\\\nModel Architecture: 3D U-Net & DiT#\\\\nSimilar to text-to-image diffusion models, U-net and Transformer are still two common architecture choices. There are a series of diffusion video modeling papers from Google based on the U-net architecture and a recent Sora model from OpenAI leveraged the Transformer architecture.\\\\nVDM (Ho & Salimans, et al. 2022) adopts the standard diffusion model setup but with an altered architecture suitable for video modeling. It extends the 2D U-net to work for 3D data (Cicek et al. 2016), where each feature map represents a 4D tensor of frames x height x width x channels. This 3D U-net is factorized over space and time, meaning that each layer only operates on the space or time dimension, but not both:\\\\n\\\\nProcessing Space:\\\\n\\\\nEach old 2D convolution layer as in the 2D U-net is extended to be space-only 3D convolution; precisely, 3x3 convolutions become 1x3x3 convolutions.\\\\nEach spatial attention block remains as attention over space, where the first axis (frames) is treated as batch dimension.\\\\n\\\\n\\\\nProcessing Time:\\', metadata={\\'description\\': \\'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\', \\'language\\': \\'en\\', \\'source\\': \\'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\\', \\'start_index\\': 7764, \\'title\\': \"Diffusion Models for Video Generation | Lil\\'Log\"}), Document(page_content=\\'Spatiotemporal SR layers contain pseudo-3D convo layers and pseudo-3D attention layers:\\\\n\\\\nPseudo-3D convo layer : Each spatial 2D convo layer (initialized from the pre-training image model) is followed by a temporal 1D layer (initialized as the identity function). Conceptually, the convo 2D layer first generates multiple frames and then frames are reshaped to be a video clip.\\\\nPseudo-3D attention layer: Following each (pre-trained) spatial attention layer, a temporal attention layer is stacked and used to approximate a full spatiotemporal attention layer.\\\\n\\\\n\\\\nFig. 7. How pseudo-3D convolution (left) and attention (right) layers work.(Image source: Singer et al. 2022)\\\\nThey can be represented as:\\\\n\\\\n$$\\\\n\\\\\\\\begin{aligned}\\\\n\\\\\\\\text{Conv}_\\\\\\\\text{P3D} &= \\\\\\\\text{Conv}_\\\\\\\\text{1D}(\\\\\\\\text{Conv}_\\\\\\\\text{2D}(\\\\\\\\mathbf{h}) \\\\\\\\circ T) \\\\\\\\circ T \\\\\\\\\\\\\\\\\\\\n\\\\\\\\text{Attn}_\\\\\\\\text{P3D} &= \\\\\\\\text{flatten}^{-1}(\\\\\\\\text{Attn}_\\\\\\\\text{1D}(\\\\\\\\text{Attn}_\\\\\\\\text{2D}(\\\\\\\\text{flatten}(\\\\\\\\mathbf{h})) \\\\\\\\circ T) \\\\\\\\circ T)\\\\n\\\\\\\\end{aligned}\\\\n$$\\\\n\\\\nwhere an input tensor $\\\\\\\\mathbf{h} \\\\\\\\in \\\\\\\\mathbb{R}^{B\\\\\\\\times C \\\\\\\\times F \\\\\\\\times H \\\\\\\\times W}$  (corresponding to batch size, channels, frames, height and weight); and $\\\\\\\\circ T$ swaps between temporal and spatial dimensions; $\\\\\\\\text{flatten}(.)$ is a matrix operator to convert $\\\\\\\\mathbf{h}$ to be $\\\\\\\\mathbf{h}‚Äô \\\\\\\\in \\\\\\\\mathbb{R}^{B \\\\\\\\times C \\\\\\\\times F \\\\\\\\times HW}$ and $\\\\\\\\text{flatten}^{-1}(.)$ reverses that process.\\\\nDuring training, different components of Make-A-Video pipeline are trained independently.\\', metadata={\\'description\\': \\'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\', \\'language\\': \\'en\\', \\'source\\': \\'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\\', \\'start_index\\': 14133, \\'title\\': \"Diffusion Models for Video Generation | Lil\\'Log\"}), Document(page_content=\"Diffusion Models for Video Generation | Lil\\'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLil\\'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nPosts\\\\n\\\\n\\\\n\\\\n\\\\nArchive\\\\n\\\\n\\\\n\\\\n\\\\nSearch\\\\n\\\\n\\\\n\\\\n\\\\nTags\\\\n\\\\n\\\\n\\\\n\\\\nFAQ\\\\n\\\\n\\\\n\\\\n\\\\nemojisearch.app\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n      Diffusion Models for Video Generation\\\\n    \\\\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\\\n\\\\n\\\\n \\\\n\\\\n\\\\nTable of Contents\\\\n\\\\n\\\\n\\\\nVideo Generation Modeling from Scratch\\\\n\\\\nParameterization & Sampling Basics\\\\n\\\\nModel Architecture: 3D U-Net & DiT\\\\n\\\\n\\\\nAdapting Image Models to Generate Videos\\\\n\\\\nFine-tuning on Video Data\\\\n\\\\nTraining-Free Adaptation\\\\n\\\\n\\\\nCitation\\\\n\\\\nReferences\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\\\n\\\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\\\n\\\\n\\\\n\\\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\", metadata={\\'description\\': \\'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\', \\'language\\': \\'en\\', \\'source\\': \\'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\\', \\'start_index\\': 6, \\'title\\': \"Diffusion Models for Video Generation | Lil\\'Log\"}), Document(page_content=\\'Fig. 10. A pre-training LDM for image synthesis is extended to be a video generator. $B, T, C, H, W$ are batch size, sequence length, channels, height and width, respectively. $\\\\\\\\mathbf{c}_S$ is an optional conditioning/context frame. (Image source: Blattmann et al. 2023)\\\\nHowever, there is a remaining issue with LDM‚Äôs pretrainined autoencoder which only sees images never videos. Naively using that for video generation can cause flickering artifacts without good temporal coherence. So Video LDM adds additional temporal layers into the decoder and fine-tuned on video data with a patch-wise temporal discriminator built from 3D convolutions, while the encoder remains unchanged so that we still can reuse the pretrained LDM. During temporal decoder fine-tuning, the frozen encoder processes each frame in the video independently, and enforce temporally coherent reconstructions across frames with a video-aware discriminator.\\\\n\\\\nFig. 11. The training pipeline of autoencoder in video latent diffusion models. The decoder is fine-tuned to have temporal coherency with a new across-frame discriminator while the encoder stays frozen. (Image source: Blattmann et al. 2023)\\\\nSimilar to Video LDM, the architecture design of Stable Video Diffusion (SVD; Blattmann et al. 2023) is also based on LDM with temporal layers inserted after every spatial convolution and attention layer, but SVD fine-tunes the entire model. There are three stages for training video LDMs:\\', metadata={\\'description\\': \\'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\', \\'language\\': \\'en\\', \\'source\\': \\'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\\', \\'start_index\\': 20363, \\'title\\': \"Diffusion Models for Video Generation | Lil\\'Log\"}), Document(page_content=\\'Sampling the sequence of latent codes with motion dynamics to keep the global scene and the background time consistent;\\\\nReprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object.\\\\n\\\\n\\\\nFig. 14. An overview of the Text2Video-Zero pipeline. (Image source: Khachatryan et al. 2023)\\\\nThe process of sampling a sequence of latent variables, $\\\\\\\\mathbf{x}^1_T, \\\\\\\\dots, \\\\\\\\mathbf{x}^m_T$, with motion information is described as follows:\\\\n\\\\nDefine a direction $\\\\\\\\boldsymbol{\\\\\\\\delta} = (\\\\\\\\delta_x, \\\\\\\\delta_y) \\\\\\\\in \\\\\\\\mathbb{R}^2$ for controlling the global scene and camera motion; by default, we set $\\\\\\\\boldsymbol{\\\\\\\\delta} = (1, 1)$. Also define a hyperparameter $\\\\\\\\lambda > 0$ controlling the amount of global motion.\\\\nFirst sample the latent code of the first frame at random, $\\\\\\\\mathbf{x}^1_T \\\\\\\\sim \\\\\\\\mathcal{N}(0, I)$;\\\\nPerform $\\\\\\\\Delta t \\\\\\\\geq 0$ DDIM backward update steps using the pre-trained image diffusion model, e.g. Stable Diffusion (SD) model in the paper, and obtain the corresponding latent code $\\\\\\\\mathbf{x}^1_{T‚Äô}$ where $T‚Äô = T - \\\\\\\\Delta t$.\\\\nFor each frame in the latent code sequence, we apply corresponding motion translation with a warping operation defined by $\\\\\\\\boldsymbol{\\\\\\\\delta}^k = \\\\\\\\lambda(k-1)\\\\\\\\boldsymbol{\\\\\\\\delta}$ to obtain $\\\\\\\\tilde{\\\\\\\\mathbf{x}}^k_{T‚Äô}$.\\\\nFinally apply DDIM forward steps to all $\\\\\\\\tilde{\\\\\\\\mathbf{x}}^{2:m}_{T‚Äô}$ to obtain $\\\\\\\\mathbf{x}^{2:m}_T$.\\', metadata={\\'description\\': \\'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\', \\'language\\': \\'en\\', \\'source\\': \\'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\\', \\'start_index\\': 24964, \\'title\\': \"Diffusion Models for Video Generation | Lil\\'Log\"})]\\n\\nQuestion: What is 3D stable disfussion?\\n\\nIf you don\\'t know the anwer, just respond \"I could not find the answer based on the provided context.\"\\n\\nAnswer: ')]\n",
      "--------------------------------------------------\n",
      "I could not find the answer based on the provided context. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is 3D stable disfussion?\"\n",
    "answer = ask_question(user_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
